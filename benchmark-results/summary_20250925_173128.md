# PGO Benchmark Results

**Date:** Thu Sep 25 05:52:34 PM EDT 2025
**Model:** /home/jeef/.inferno/models/TinyLlama_TinyLlama-1.1B-Chat-v1.0/model.safetensors
**Timestamp:** 20250925_173128

## Binary Information

| Binary | Size |
|--------|------|
| Baseline | 16MB |
| PGO | 14MB |


## Results

### Criterion.rs Results
- Detailed results: `./target/criterion/report/index.html`
- Raw logs: `/home/jeef/inferno/benchmark-results/criterion_pgo_20250925_173128.log`

### Hyperfine Results
- JSON results: `/home/jeef/inferno/benchmark-results/hyperfine_*_20250925_173128.json`
- Markdown reports: `/home/jeef/inferno/benchmark-results/hyperfine_*_20250925_173128.md`


## Analysis

The benchmarks compare:
1. **Baseline**: Standard release build with `-C target-cpu=native`
2. **PGO**: Profile-guided optimization build with comprehensive profiling

### Key Metrics
- **Inference Latency**: Time to generate response for various prompt types
- **Cold Start**: Model loading + first inference time
- **Throughput**: Batch processing performance

### Expected Improvements
PGO typically provides:
- 10-30% reduction in inference latency
- Better branch prediction for hot paths
- Improved cache locality for frequent operations
- Optimized tokenization and attention computation paths

