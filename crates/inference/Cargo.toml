[package]
name = "inferno-inference"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Inference engine implementation for Inferno with Burn ML framework support"

[dependencies]
# Workspace dependencies
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tracing = { workspace = true }
thiserror = { workspace = true }
async-trait = { workspace = true }
chrono = { workspace = true }

# Local dependencies
inferno-shared = { workspace = true }

# Configuration and environment
config = { workspace = true }
toml = { workspace = true }
num_cpus = { workspace = true }

# JSON and validation
validator = { workspace = true }

# Process execution is handled by std::process (no additional crate needed)

# ML Framework dependencies - Always enabled for simplicity
# (removing feature complexity as requested)

# Candle ML Framework for optimized inference (with CUDA 12.9 support)
candle-core = { workspace = true }
candle-nn = { workspace = true }
candle-transformers = { workspace = true }

# Shared dependencies for ML frameworks
tokenizers = { workspace = true }
hf-hub = { workspace = true }
safetensors = { workspace = true }
bytemuck = { workspace = true }
half = { workspace = true }

# Burn ML Framework - commented out due to compilation issues
# burn = { workspace = true }
# burn-import = { workspace = true }
# llama-burn = { workspace = true }


[dev-dependencies]
tokio-test = { workspace = true }
criterion = { workspace = true }
proptest = { workspace = true }
serial_test = { workspace = true }
tempfile = { workspace = true }
rstest = { workspace = true }


# Benchmarks will be added in later phases
# [[bench]]
# name = "latency"
# harness = false

# [[bench]]
# name = "throughput"
# harness = false
