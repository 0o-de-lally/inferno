//! Type definitions and conversions between Rust and C FFI

#![allow(missing_docs)]

use crate::error::{VLLMError, VLLMResult};
use serde::{Deserialize, Serialize};
use std::ffi::{CStr, CString};
use std::os::raw::c_char;

/// Rust representation of inference request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    pub prompt: String,
    pub max_tokens: usize,
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: i32,
    pub stop_sequences: Vec<String>,
    pub stream: bool,
    pub request_id: u64,
}

impl Default for InferenceRequest {
    fn default() -> Self {
        Self {
            prompt: String::new(),
            max_tokens: 100,
            temperature: 1.0,
            top_p: 1.0,
            top_k: -1,
            stop_sequences: Vec::new(),
            stream: false,
            request_id: 0,
        }
    }
}

/// Rust representation of inference response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResponse {
    pub request_id: u64,
    pub generated_text: String,
    pub generated_tokens: usize,
    pub is_finished: bool,
    pub error_message: Option<String>,
    pub inference_time_ms: f64,
    pub queue_time_ms: f64,
    pub total_tokens: usize,
    pub prompt_tokens: usize,
}

impl Default for InferenceResponse {
    fn default() -> Self {
        Self {
            request_id: 0,
            generated_text: String::new(),
            generated_tokens: 0,
            is_finished: false,
            error_message: None,
            inference_time_ms: 0.0,
            queue_time_ms: 0.0,
            total_tokens: 0,
            prompt_tokens: 0,
        }
    }
}

/// Rust representation of memory statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryStats {
    pub total_memory_bytes: usize,
    pub used_memory_bytes: usize,
    pub free_memory_bytes: usize,
    pub cached_memory_bytes: usize,
    pub fragmentation_bytes: usize,
    pub utilization_percentage: f64,
    pub device_id: i32,
}

impl Default for MemoryStats {
    fn default() -> Self {
        Self {
            total_memory_bytes: 0,
            used_memory_bytes: 0,
            free_memory_bytes: 0,
            cached_memory_bytes: 0,
            fragmentation_bytes: 0,
            utilization_percentage: 0.0,
            device_id: 0,
        }
    }
}

// FFI type definitions (these would normally be generated by bindgen)
#[cfg(feature = "cuda")]
#[repr(C)]
#[derive(Debug, Copy, Clone, PartialEq, Eq)]
#[allow(missing_docs, non_camel_case_types)]
pub enum VLLMErrorCode {
    VLLM_SUCCESS = 0,
    VLLM_ERROR_INVALID_ARGUMENT = 1,
    VLLM_ERROR_OUT_OF_MEMORY = 2,
    VLLM_ERROR_CUDA_ERROR = 3,
    VLLM_ERROR_MODEL_NOT_LOADED = 4,
    VLLM_ERROR_INFERENCE_FAILED = 5,
    VLLM_ERROR_INITIALIZATION_FAILED = 6,
    VLLM_ERROR_SHUTDOWN_FAILED = 7,
}

#[cfg(feature = "cuda")]
pub type VLLMEngineHandle = *mut std::ffi::c_void;

#[cfg(feature = "cuda")]
pub type VLLMRequestHandle = *mut std::ffi::c_void;

#[cfg(feature = "cuda")]
#[repr(C)]
#[allow(missing_docs)]
pub struct VLLMConfig {
    pub model_path: *const c_char,
    pub model_name: *const c_char,
    pub device_id: i32,
    pub max_batch_size: usize,
    pub max_sequence_length: usize,
    pub max_tokens: usize,
    pub gpu_memory_pool_size_mb: usize,
    pub max_num_seqs: usize,
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: i32,
    pub worker_threads: usize,
    pub enable_async_processing: bool,
}

#[cfg(feature = "cuda")]
impl Default for VLLMConfig {
    fn default() -> Self {
        Self {
            model_path: std::ptr::null(),
            model_name: std::ptr::null(),
            device_id: 0,
            max_batch_size: 8,
            max_sequence_length: 4096,
            max_tokens: 100,
            gpu_memory_pool_size_mb: 4096,
            max_num_seqs: 256,
            temperature: 1.0,
            top_p: 1.0,
            top_k: -1,
            worker_threads: 4,
            enable_async_processing: true,
        }
    }
}

#[cfg(feature = "cuda")]
impl VLLMConfig {
    /// Convert from Rust config to FFI config
    pub fn from_rust_config(rust_config: &crate::config::VLLMConfig) -> VLLMResult<Self> {
        let model_path = CString::new(rust_config.model_path.as_str())
            .map_err(|_| VLLMError::InvalidArgument("Invalid model path".to_string()))?;
        let model_name = CString::new(rust_config.model_name.as_str())
            .map_err(|_| VLLMError::InvalidArgument("Invalid model name".to_string()))?;

        // Note: These CStrings need to be kept alive, so in real implementation
        // we'd store them in a container that lives as long as the VLLMConfig
        let model_path_ptr = model_path.as_ptr();
        let model_name_ptr = model_name.as_ptr();

        // Prevent the CStrings from being dropped
        std::mem::forget(model_path);
        std::mem::forget(model_name);

        Ok(Self {
            model_path: model_path_ptr,
            model_name: model_name_ptr,
            device_id: rust_config.device_id,
            max_batch_size: rust_config.max_batch_size,
            max_sequence_length: rust_config.max_sequence_length,
            max_tokens: rust_config.max_tokens,
            gpu_memory_pool_size_mb: rust_config.gpu_memory_pool_size_mb,
            max_num_seqs: rust_config.max_num_seqs,
            temperature: rust_config.temperature,
            top_p: rust_config.top_p,
            top_k: rust_config.top_k,
            worker_threads: rust_config.worker_threads,
            enable_async_processing: rust_config.enable_async_processing,
        })
    }
}

#[cfg(feature = "cuda")]
#[repr(C)]
#[allow(missing_docs)]
pub struct VLLMInferenceRequest {
    pub prompt: *const c_char,
    pub max_tokens: usize,
    pub temperature: f32,
    pub top_p: f32,
    pub top_k: i32,
    pub stop_sequences: *const c_char,
    pub stream: bool,
    pub request_id: u64,
}

#[cfg(feature = "cuda")]
impl Default for VLLMInferenceRequest {
    fn default() -> Self {
        Self {
            prompt: std::ptr::null(),
            max_tokens: 100,
            temperature: 1.0,
            top_p: 1.0,
            top_k: -1,
            stop_sequences: std::ptr::null(),
            stream: false,
            request_id: 0,
        }
    }
}

#[cfg(feature = "cuda")]
impl VLLMInferenceRequest {
    /// Convert from Rust request to FFI request
    pub fn from_rust_request(request: &InferenceRequest) -> VLLMResult<Self> {
        let prompt = CString::new(request.prompt.as_str())
            .map_err(|_| VLLMError::InvalidArgument("Invalid prompt".to_string()))?;

        let stop_sequences = if request.stop_sequences.is_empty() {
            CString::new("")
        } else {
            CString::new(request.stop_sequences.join("\n"))
        }
        .map_err(|_| VLLMError::InvalidArgument("Invalid stop sequences".to_string()))?;

        let prompt_ptr = prompt.as_ptr();
        let stop_sequences_ptr = stop_sequences.as_ptr();

        // Prevent strings from being dropped
        std::mem::forget(prompt);
        std::mem::forget(stop_sequences);

        Ok(Self {
            prompt: prompt_ptr,
            max_tokens: request.max_tokens,
            temperature: request.temperature,
            top_p: request.top_p,
            top_k: request.top_k,
            stop_sequences: stop_sequences_ptr,
            stream: request.stream,
            request_id: request.request_id,
        })
    }
}

#[cfg(feature = "cuda")]
#[repr(C)]
#[allow(missing_docs)]
pub struct VLLMInferenceResponse {
    pub request_id: u64,
    pub generated_text: *const c_char,
    pub generated_tokens: usize,
    pub is_finished: bool,
    pub error_code: VLLMErrorCode,
    pub error_message: *const c_char,
    pub inference_time_ms: f64,
    pub queue_time_ms: f64,
    pub total_tokens: usize,
    pub prompt_tokens: usize,
}

#[cfg(feature = "cuda")]
impl Default for VLLMInferenceResponse {
    fn default() -> Self {
        Self {
            request_id: 0,
            generated_text: std::ptr::null(),
            generated_tokens: 0,
            is_finished: false,
            error_code: VLLMErrorCode::VLLM_SUCCESS,
            error_message: std::ptr::null(),
            inference_time_ms: 0.0,
            queue_time_ms: 0.0,
            total_tokens: 0,
            prompt_tokens: 0,
        }
    }
}

#[cfg(feature = "cuda")]
impl InferenceResponse {
    /// Convert from FFI response to Rust response
    pub fn from_c_response(c_response: &VLLMInferenceResponse) -> VLLMResult<Self> {
        let generated_text = if c_response.generated_text.is_null() {
            String::new()
        } else {
            unsafe {
                CStr::from_ptr(c_response.generated_text)
                    .to_string_lossy()
                    .to_string()
            }
        };

        let error_message = if c_response.error_message.is_null() {
            None
        } else {
            Some(unsafe {
                CStr::from_ptr(c_response.error_message)
                    .to_string_lossy()
                    .to_string()
            })
        };

        Ok(Self {
            request_id: c_response.request_id,
            generated_text,
            generated_tokens: c_response.generated_tokens,
            is_finished: c_response.is_finished,
            error_message,
            inference_time_ms: c_response.inference_time_ms,
            queue_time_ms: c_response.queue_time_ms,
            total_tokens: c_response.total_tokens,
            prompt_tokens: c_response.prompt_tokens,
        })
    }
}

#[cfg(feature = "cuda")]
#[repr(C)]
#[allow(missing_docs)]
pub struct VLLMMemoryStats {
    pub total_memory_bytes: usize,
    pub used_memory_bytes: usize,
    pub free_memory_bytes: usize,
    pub cached_memory_bytes: usize,
    pub fragmentation_bytes: usize,
    pub utilization_percentage: f64,
    pub device_id: i32,
}

#[cfg(feature = "cuda")]
impl Default for VLLMMemoryStats {
    fn default() -> Self {
        Self {
            total_memory_bytes: 0,
            used_memory_bytes: 0,
            free_memory_bytes: 0,
            cached_memory_bytes: 0,
            fragmentation_bytes: 0,
            utilization_percentage: 0.0,
            device_id: 0,
        }
    }
}

#[cfg(feature = "cuda")]
impl MemoryStats {
    /// Convert from FFI stats to Rust stats
    pub fn from_c_stats(c_stats: &VLLMMemoryStats) -> Self {
        Self {
            total_memory_bytes: c_stats.total_memory_bytes,
            used_memory_bytes: c_stats.used_memory_bytes,
            free_memory_bytes: c_stats.free_memory_bytes,
            cached_memory_bytes: c_stats.cached_memory_bytes,
            fragmentation_bytes: c_stats.fragmentation_bytes,
            utilization_percentage: c_stats.utilization_percentage,
            device_id: c_stats.device_id,
        }
    }
}
