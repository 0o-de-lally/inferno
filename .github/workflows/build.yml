name: Build

on:
  schedule:
    - cron: "45 0 * * *" # Run at 00:45 every day
  push:
    branches: ["**"]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always

jobs:
  detect-gpu:
    name: GPU Detection
    runs-on: ubuntu-latest
    outputs:
      cuda_available: ${{ steps.gpu.outputs.available }}
    steps:
    - name: Check for NVIDIA GPU
      id: gpu
      run: |
        if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
          echo "available=true" >> $GITHUB_OUTPUT
          echo "Physical GPU detected"
        else
          echo "available=false" >> $GITHUB_OUTPUT
          echo "No physical GPU detected"
        fi

  cuda-build:
    name: CUDA Build (Primary Target)
    runs-on: ubuntu-latest
    needs: detect-gpu
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache Rust dependencies
      uses: Swatinem/rust-cache@v2
      with:
        cache-on-failure: true

    - name: Update dependencies for CUDA 13.0 support
      run: |
        echo "Updating dependencies to ensure cudarc 0.17.3+ for CUDA 13.0 support..."
        cargo update

    - name: Setup CUDA
      uses: Jimver/cuda-toolkit@v0.2.27
      with:
        cuda: '13.0.0'
        method: 'network'
        sub-packages: '["nvcc", "cudart", "cudart-dev", "thrust", "nvidia-fs", "libcublas", "libcublas-dev"]'

    - name: Install NVIDIA Utils
      run: |
        # Install nvidia-utils package which provides nvidia-smi
        sudo apt-get update
        sudo apt-get install -y nvidia-utils-535 || sudo apt-get install -y nvidia-utils-525 || sudo apt-get install -y nvidia-utils-520
        # Create a mock nvidia-smi if the real one doesn't work in CI
        if ! command -v nvidia-smi &> /dev/null; then
          echo "Creating mock nvidia-smi for build compatibility"
          sudo mkdir -p /usr/bin
          sudo tee /usr/bin/nvidia-smi > /dev/null << 'EOF'
          #!/bin/bash
          # Mock nvidia-smi for CI builds
          echo "NVIDIA-SMI 535.54.03    Driver Version 535.54.03    CUDA Version 13.0"
          echo ""
          echo "| NVIDIA-SMI 535.54.03              Driver Version 535.54.03    CUDA Version 13.0     |"
          echo "|-------------------------------+----------------------+----------------------+"
          echo "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |"
          echo "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |"
          echo "|                               |                      |               MIG M. |"
          echo "|===============================+======================+======================|"
          echo "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |"
          echo "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |"
          echo "|                               |                      |                  N/A |"
          echo "+-------------------------------+----------------------+----------------------+"
          EOF
          sudo chmod +x /usr/bin/nvidia-smi
        fi

    - name: Verify CUDA Installation
      run: |
        echo "CUDA Installation Verification:"
        nvcc --version
        echo "CUDA_HOME: $CUDA_HOME"
        echo "CUDA_PATH: $CUDA_PATH"
        ls -la $CUDA_PATH/lib64/ | head -10
        echo "NVIDIA-SMI verification:"
        nvidia-smi --version || echo "nvidia-smi version check failed"
        nvidia-smi || echo "nvidia-smi GPU query failed (expected in CI without actual GPU)"

    - name: Build CUDA Binary
      env:
        # CUDA environment variables (set by cuda-toolkit action)
        LD_LIBRARY_PATH: "${{ env.CUDA_PATH }}/lib64:/usr/lib/x86_64-linux-gnu:${{ env.LD_LIBRARY_PATH }}"
      run: |
        echo "Building CUDA binary for production release"

        # Show environment for debugging
        echo "CUDA Environment:"
        echo "CUDA_HOME=$CUDA_HOME"
        echo "CUDA_PATH=$CUDA_PATH"
        echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
        echo "PATH includes CUDA: $(echo $PATH | grep cuda || echo 'No CUDA in PATH')"
        nvcc --version 2>/dev/null || echo "nvcc not available"

        echo "Building CUDA backend with full development environment"
        # Build with verbose output to catch any compilation issues
        cargo build --release --features candle-cuda --verbose


