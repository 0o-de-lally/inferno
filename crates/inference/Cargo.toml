[package]
name = "inferno-inference"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Inference engine implementation for Inferno with Burn ML framework support"

[dependencies]
# Workspace dependencies
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tracing = { workspace = true }
thiserror = { workspace = true }
async-trait = { workspace = true }
chrono = { workspace = true }

# Local dependencies
inferno-shared = { path = "../shared" }

# HTTP server

# Minimal dependencies for real model inference (avoiding edition2024 conflicts)
reqwest = { version = "0.11", optional = true }
# Note: Avoiding tokenizers/hf-hub until toolchain upgrade, using workspace serde_json

# FFI and build dependencies

# Configuration and environment
config = "0.14"
toml = "0.8"
num_cpus = "1.16"

# Async utilities

# Memory management
parking_lot = "0.12"
once_cell = "1.19"

# Performance monitoring

# JSON and validation
validator = { version = "0.18", features = ["derive"] }

# Tensor operations (for host-side work)

# No build dependencies needed for pure Rust Burn framework implementation

[dev-dependencies]
tokio-test = { workspace = true }
criterion = { workspace = true }
proptest = { workspace = true }
serial_test = { workspace = true }
reqwest = { workspace = true }
tempfile = "3.8"
rstest = "0.18"

[features]
default = ["burn-cpu"]
burn-cpu = ["reqwest"]  # Burn framework CPU backend (minimal)
burn-cuda = ["reqwest"]  # Burn framework CUDA backend (minimal)  
cuda = []  # Reserved for future use

# Benchmarks will be added in later phases
# [[bench]]
# name = "latency" 
# harness = false

# [[bench]]
# name = "throughput"
# harness = false